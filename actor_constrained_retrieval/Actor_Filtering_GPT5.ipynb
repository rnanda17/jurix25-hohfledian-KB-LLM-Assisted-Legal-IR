{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e48f5d0-18d2-4149-9296-a50dadebc5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, json, unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "from openai import OpenAI\n",
    "import requests\n",
    "from litellm import token_counter\n",
    "from typing import List, Dict\n",
    "from sklearn.metrics import ndcg_score, average_precision_score\n",
    "from nltk.metrics.scores import precision as nltk_precision, recall as nltk_recall, f_measure as nltk_f1\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#open router key\n",
    "os.environ[\"OPENROUTER_API_KEY\"] = \"set your key\"\n",
    "\n",
    "# --- Client (OpenAI-compatible) ---\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=os.environ[\"OPENROUTER_API_KEY\"],\n",
    ")\n",
    "\n",
    "MODEL_NAME = \"openai/gpt-5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26a200ba-8f14-4400-b793-b056068d7312",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the master mapping file (fixed_mappings_with_nls_new.csv) as we will only use filtered rows for each question in this code\n",
    "ACTOR_CSV   = \"actor_defintions.csv\"\n",
    "CSV_path_questions = \"15_questions_ground_truth.csv\"\n",
    "\n",
    "\n",
    "# --- NEW (per-question filtered files) ---\n",
    "# Directory with: question_1_filtered_rows.csv ... question_15_filtered_rows.csv\n",
    "FILTERED_MAPPINGS_DIR = \"/final_files/results_actor_filtering_HW/gpt-5_high/\"\n",
    "FILTERED_FILENAME_TEMPLATE = \"question_{n}_filtered_rows.csv\"\n",
    "\n",
    "\n",
    "# regex to extract UID-like tokens as a fallback\n",
    "UID_REGEX = re.compile(r\"[A-Za-z]+_[A-Za-z0-9]+\")\n",
    "\n",
    "\n",
    "# unchanged from normal retrieval pipeline\n",
    "def _parse_gold_ids_robust(raw: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Parse `gold_unique_ids` into a list[str].\n",
    "    Handles:\n",
    "      - Proper JSON lists\n",
    "      - Comma-separated strings with stray quotes/brackets\n",
    "      - Messy strings by regex fallback\n",
    "    \"\"\"\n",
    "    if raw is None:\n",
    "        return []\n",
    "    s = str(raw).strip() # if not string, then convert to string and remove trailing whitespace\n",
    "\n",
    "    # 1) Try JSON directly\n",
    "    try:\n",
    "        if s.startswith(\"[\") and s.endswith(\"]\"):\n",
    "            data = json.loads(s)\n",
    "            return [str(x).strip() for x in data if str(x).strip()]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 2) Try manual splitting if JSON fails\n",
    "    try:\n",
    "        ss = s.replace(\"'\", '\"')\n",
    "        # Sometimes the whole list is double-quoted\n",
    "        if ss.startswith('\"') and ss.endswith('\"'):\n",
    "            ss = ss[1:-1]\n",
    "        if ss.startswith(\"[\") and ss.endswith(\"]\"):\n",
    "            return [\n",
    "                x.strip().strip('\"').strip(\"'\")\n",
    "                for x in ss[1:-1].split(\",\")\n",
    "                if x.strip().strip('\"').strip(\"'\")\n",
    "            ]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 3) Regex fallback: just grab UID-like tokens\n",
    "    matches = UID_REGEX.findall(s)\n",
    "    return matches\n",
    "\n",
    "\n",
    "\n",
    "# unchanged from normal retrieval pipeline\n",
    "def load_questions_csv_robust(path: str) -> List[Dict[str, object]]:\n",
    "    \"\"\"\n",
    "    Load questions from a ';'-separated CSV with columns:\n",
    "      question_id, question_text, gold_unique_ids\n",
    "    Returns a list of dicts in the same format your pipeline expects:\n",
    "      {\"question_id\": str, \"question_text\": str, \"gold_unique_ids\": List[str]}\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path, sep=\";\", dtype=str) #separator is ;, reading all columns as string\n",
    "\n",
    "    # Case-insensitive column mapping\n",
    "    cols_map = {c.lower(): c for c in df.columns} #lowercase the columns \n",
    "    \n",
    "    qid_col = cols_map[\"question_id\"]\n",
    "    qtext_col = cols_map[\"question_text\"]\n",
    "    gold_col = cols_map[\"gold_unique_ids\"]\n",
    "\n",
    "    questions: List[Dict[str, object]] = []\n",
    "    for _, row in df.iterrows():\n",
    "        qid = str(row[qid_col]).strip()\n",
    "        qtext = str(row[qtext_col]).strip()\n",
    "        gold_list = _parse_gold_ids_robust(row[gold_col])\n",
    "        \n",
    "        questions.append({\n",
    "            \"question_id\": qid,\n",
    "            \"question_text\": qtext,\n",
    "            \"gold_unique_ids\": gold_list,\n",
    "        })\n",
    "    return questions\n",
    "\n",
    "\n",
    "\n",
    "# --------------- OPENROUTER CLIENT + LiteLLM TOKEN COUNT ---------------\n",
    "\n",
    "\n",
    "# LiteLLM token_counter (model-aware)\n",
    "# unchanged from normal retrieval pipeline\n",
    "def estimate_tokens_with_litellm(messages, model_id: str) -> int:\n",
    "    \"\"\"Return the number of prompt tokens as estimated by LiteLLM.\n",
    "    Prompt tokens are the tokens that you input into the model. This is the number of tokens in your prompt.\n",
    "    \"\"\"\n",
    "    return token_counter(model=model_id, messages=messages)\n",
    "\n",
    "# --------------- DATA LOADING & NORMALIZATION ---------------\n",
    "# unchanged from normal retrieval pipeline\n",
    "def _norm(s):\n",
    "    if s is None: return \"\"\n",
    "    s = str(s)\n",
    "    s = unicodedata.normalize(\"NFKC\", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "# --- Load actor definitions from CSV ---\n",
    "# unchanged from normal retrieval pipeline\n",
    "def load_actor_defs_csv(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load actor definitions from a CSV file with 2 columns:\n",
    "      - actor_name\n",
    "      - actor_definition\n",
    "    Returns DataFrame with those two columns.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path, dtype=str,  sep=\";\")   # ensure all text\n",
    "    return df\n",
    "\n",
    "\n",
    "# --- Convert actor definitions into a string for the LLM ---\n",
    "# unchanged from normal retrieval pipeline\n",
    "def actor_defs_to_text(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Convert the actor definitions DataFrame into a neat text block\n",
    "    to inject into the LLM prompt.\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    for _, row in df.iterrows():\n",
    "        lines.append(f\"{row['actor_name']}: {row['actor_definition']}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "# unchanged from normal retrieval pipeline\n",
    "def build_mapping_lines(df: pd.DataFrame) -> list[str]:\n",
    "    lines = []\n",
    "    for _, r in df.iterrows():\n",
    "        uid = str(r[\"Unique_ID\"])\n",
    "        prv = r[\"provision\"]\n",
    "        nls = r[\"natural_language_sentence\"]\n",
    "        lines.append(f\"[UID={uid}] (Provision: {prv}) {nls}\")\n",
    "    return lines\n",
    "\n",
    "\n",
    "actors_df   = load_actor_defs_csv(ACTOR_CSV)\n",
    "ACTORS_TEXT = actor_defs_to_text(actors_df)  \n",
    "questions = load_questions_csv_robust(CSV_path_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89c899e8-502e-4448-968e-28fa16cf16a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PIPELINE START ===\n",
      "\n",
      "--- Processing Question Q1 ---\n",
      "[Filtered] Using: /Users/rohanpersonal/Desktop/jurix_paper/normalized_mappings/final_files/results_actor_filtering_HW/gpt-5_high/question_1_filtered_rows.csv (rows=361)\n",
      "\n",
      "[Step 1] Retrieving predicted UIDs...\n",
      "[Prompt mappings] 361 lines\n",
      "\n",
      "[Server usage] prompt_tokens = 25600, completion_tokens = 9951, total_tokens = 35551\n",
      "\n",
      "[Step 2] Evaluating predictions...\n",
      "\n",
      "=== Q1 ===\n",
      "Question: An Australian biotechnology company conducts a private expedition to collect marine genetic resources (MGRs) in areas beyond national jurisdiction (ABNJ). The collected materials are sequenced in-house, and the resulting digital sequence information (DSI) is published in open-access repositories. The same company later relies on this DSI to develop a commercially valuable enzyme. No prior informed consent (PIC) or mutually agreed terms (MAT) were established, and no monetary or non-monetary benefit-sharing has occurred. The company argues that because the material was collected in ABNJ and only DSI is being used, rather than the physical samples, no benefit-sharing obligations apply under current international law. The competent authority of Australia has not taken regulatory steps to monitor or require disclosure of the activity. Question: Identify substantive provisions that allocate legal positions to the user company in relation to the access, publication, and commercial utilisation of DSI derived from MGRs collected in ABNJ, including obligations concerning benefit-sharing.\n",
      "Gold (9): ['BBNJ_M020', 'BBNJ_M035', 'BBNJ_M043', 'BBNJ_M044', 'BBNJ_M045', 'BBNJ_M046', 'BBNJ_M047', 'BBNJ_M049', 'BBNJ_M136']\n",
      "Preds(3): ['BBNJ_M035', 'BBNJ_M045', 'BBNJ_M046']\n",
      "\n",
      "[NLTK metrics]\n",
      " Precision=1.000, Recall=0.333, F1=0.500\n",
      "\n",
      "[Manual metrics from TP/FP/FN]\n",
      " Precision=1.000, Recall=0.333, F1=0.500\n",
      " Counts -> TP=3, FP=0, FN=6\n",
      "\n",
      "--- Processing Question Q2 ---\n",
      "[Filtered] Using: /Users/rohanpersonal/Desktop/jurix_paper/normalized_mappings/final_files/results_actor_filtering_HW/gpt-5_high/question_2_filtered_rows.csv (rows=270)\n",
      "\n",
      "[Step 1] Retrieving predicted UIDs...\n",
      "[Prompt mappings] 270 lines\n",
      "\n",
      "[Server usage] prompt_tokens = 20074, completion_tokens = 8494, total_tokens = 28568\n",
      "\n",
      "[Step 2] Evaluating predictions...\n",
      "\n",
      "=== Q2 ===\n",
      "Question: A university-affiliated researcher from Brazil applies for access to marine genetic resources (MGRs) located within the exclusive economic zone (EEZ) of Argentina. Both Brazil and Argentina are Parties to the Convention on Biological Diversity (CBD) and the Nagoya Protocol. The species in question are found in an area inhabited by an Indigenous community with longstanding custodianship of the surrounding marine environment and traditional knowledge (TK) associated with the biological properties of the target organisms. The university-affiliated researcher secures an access permit from Argentina’s national competent authority, which authorises sample collection for academic purposes. However, the researcher does not engage with the Indigenous community, nor does the permit process include consultation or prior informed consent (PIC) from the TK holders. No mutually agreed terms (MAT) are established, and no arrangements are made for benefit-sharing with the community. The research proceeds, and samples are exported to Brazil for analysis. Question: Identify substantive provisions that establish whether access to MGRs and associated TK within national jurisdiction requires engagement with Indigenous Peoples and Local Communities\n",
      "Gold (7): ['CBD_M019', 'NAGOYA_M008', 'NAGOYA_M013', 'NAGOYA_M014', 'NAGOYA_M016', 'NAGOYA_M026', 'NAGOYA_M048']\n",
      "Preds(5): ['CBD_M019', 'NAGOYA_M006', 'NAGOYA_M008', 'NAGOYA_M014', 'NAGOYA_M016']\n",
      "\n",
      "[NLTK metrics]\n",
      " Precision=0.800, Recall=0.571, F1=0.667\n",
      "\n",
      "[Manual metrics from TP/FP/FN]\n",
      " Precision=0.800, Recall=0.571, F1=0.667\n",
      " Counts -> TP=4, FP=1, FN=3\n",
      "\n",
      "--- Processing Question Q3 ---\n",
      "[Filtered] Using: /Users/rohanpersonal/Desktop/jurix_paper/normalized_mappings/final_files/results_actor_filtering_HW/gpt-5_high/question_3_filtered_rows.csv (rows=270)\n",
      "\n",
      "[Step 1] Retrieving predicted UIDs...\n",
      "[Prompt mappings] 270 lines\n",
      "\n",
      "[Server usage] prompt_tokens = 20107, completion_tokens = 9619, total_tokens = 29726\n",
      "\n",
      "[Step 2] Evaluating predictions...\n",
      "\n",
      "=== Q3 ===\n",
      "Question: A publicly funded research institution based in Peru initiates a project involving the development of bioactive compounds derived from marine genetic resources (MGRs) located within Brazil’s national jurisdiction. The Peruvian researchers rely on traditional knowledge (TK) obtained from an Indigenous community in Brazil, whose members provided ethnobiological information that guided the selection of marine species and the methodology for compound extraction. The project is formally classified as non-commercial research, and access is authorised by Brazil’s national competent authority under that designation. Accordingly, mutually agreed terms (MAT) are limited to standard reporting obligations, and no benefit-sharing mechanisms are triggered at the point of access. Several years later, the compounds developed under the project attracted substantial interest from a Peruvian pharmaceutical company, which began developing and patenting commercial products based on the initial research. Since the project was never required to track downstream uses or disclose transitions from non-commercial to commercial applications, the Indigenous community receives no benefits and remains unaware of the commercialisation pathway. Question: Identify substantive provisions  that govern obligations to share benefits arising from downstream commercial use of MGRs and associated TK, including whether a non-commercial classification at the point of access limits or extinguishes benefit-sharing duties under the Nagoya Protocol.\n",
      "Gold (17): ['NAGOYA_M003', 'NAGOYA_M006', 'NAGOYA_M011', 'NAGOYA_M013', 'NAGOYA_M015', 'NAGOYA_M016', 'NAGOYA_M019', 'NAGOYA_M045', 'NAGOYA_M046', 'NAGOYA_M047', 'NAGOYA_M048', 'NAGOYA_M051', 'NAGOYA_M052', 'NAGOYA_M053', 'NAGOYA_M054', 'NAGOYA_M055', 'NAGOYA_M056']\n",
      "Preds(10): ['NAGOYA_M003', 'NAGOYA_M006', 'NAGOYA_M015', 'NAGOYA_M019', 'NAGOYA_M045', 'NAGOYA_M046', 'NAGOYA_M048', 'NAGOYA_M049', 'NAGOYA_M051', 'NAGOYA_M052']\n",
      "\n",
      "[NLTK metrics]\n",
      " Precision=0.900, Recall=0.529, F1=0.667\n",
      "\n",
      "[Manual metrics from TP/FP/FN]\n",
      " Precision=0.900, Recall=0.529, F1=0.667\n",
      " Counts -> TP=9, FP=1, FN=8\n",
      "\n",
      "--- Processing Question Q4 ---\n",
      "[Filtered] Using: /Users/rohanpersonal/Desktop/jurix_paper/normalized_mappings/final_files/results_actor_filtering_HW/gpt-5_high/question_4_filtered_rows.csv (rows=331)\n",
      "\n",
      "[Step 1] Retrieving predicted UIDs...\n",
      "[Prompt mappings] 331 lines\n",
      "\n",
      "[Server usage] prompt_tokens = 23931, completion_tokens = 11877, total_tokens = 35808\n",
      "\n",
      "[Step 2] Evaluating predictions...\n",
      "\n",
      "=== Q4 ===\n",
      "Question: A multinational research consortium involving institutions from Portugal, Cape Verde, and Brazil undertakes a marine sampling expedition to collect genetic resources from two distinct zones: first, from within the exclusive economic zone (EEZ) of India, and second, from an adjacent area located beyond national jurisdiction (ABNJ). The researchers do not formalise any access agreements with Indian authorities but base their sampling design and species selection on traditional knowledge (TK) informally provided by a coastal Indigenous community in India. The expedition results in a large volume of digital sequence information (DSI), which is later deposited in open-access repositories. A private biotechnology company based in Belgium, unaffiliated with the consortium, accesses the DSI and uses it to synthesise a compound that becomes the active ingredient in a commercially successful cosmetic product. The commercial viability of the compound depends on the combination of genetic traits found in samples from both zones, as well as on the TK that initially guided the identification of promising species. No prior informed consent (PIC) was obtained from the Indigenous community, and no benefit-sharing, neither monetary nor non-monetary, was arranged with either the community or the Indian national competent authority. Question: Identify substantive provisions that establish obligations to share benefits from utilisation of marine genetic resources and associated traditional knowledge collected across EEZ and ABNJ boundaries, including provisions applicable to the publication and commercial use of DSI under the CBD, the Nagoya Protocol, and the BBNJ Agreement.\n",
      "Gold (50): ['BBNJ_M021', 'BBNJ_M022', 'BBNJ_M023', 'BBNJ_M024', 'BBNJ_M025', 'BBNJ_M026', 'BBNJ_M027', 'BBNJ_M028', 'BBNJ_M029', 'BBNJ_M030', 'BBNJ_M031', 'BBNJ_M032', 'BBNJ_M033', 'BBNJ_M035', 'BBNJ_M037', 'BBNJ_M038', 'BBNJ_M039', 'BBNJ_M040', 'BBNJ_M043', 'BBNJ_M044', 'BBNJ_M045', 'BBNJ_M046', 'BBNJ_M047', 'BBNJ_M049', 'BBNJ_M050', 'CBD_M019', 'CBD_M044', 'CBD_M046', 'CBD_M047', 'CBD_M049', 'NAGOYA_M003', 'NAGOYA_M004', 'NAGOYA_M006', 'NAGOYA_M008', 'NAGOYA_M011', 'NAGOYA_M013', 'NAGOYA_M014', 'NAGOYA_M015', 'NAGOYA_M016', 'NAGOYA_M026', 'NAGOYA_M045', 'NAGOYA_M046', 'NAGOYA_M047', 'NAGOYA_M048', 'NAGOYA_M051', 'NAGOYA_M052', 'NAGOYA_M053', 'NAGOYA_M054', 'NAGOYA_M055', 'NAGOYA_M056']\n",
      "Preds(17): ['BBNJ_M035', 'BBNJ_M037', 'BBNJ_M038', 'BBNJ_M039', 'BBNJ_M040', 'BBNJ_M041', 'BBNJ_M043', 'BBNJ_M044', 'BBNJ_M045', 'BBNJ_M047', 'CBD_M019', 'CBD_M049', 'NAGOYA_M003', 'NAGOYA_M006', 'NAGOYA_M016', 'NAGOYA_M045', 'NAGOYA_M051']\n",
      "\n",
      "[NLTK metrics]\n",
      " Precision=0.941, Recall=0.320, F1=0.478\n",
      "\n",
      "[Manual metrics from TP/FP/FN]\n",
      " Precision=0.941, Recall=0.320, F1=0.478\n",
      " Counts -> TP=16, FP=1, FN=34\n",
      "\n",
      "--- Processing Question Q5 ---\n",
      "[Filtered] Using: /Users/rohanpersonal/Desktop/jurix_paper/normalized_mappings/final_files/results_actor_filtering_HW/gpt-5_high/question_5_filtered_rows.csv (rows=275)\n",
      "\n",
      "[Step 1] Retrieving predicted UIDs...\n",
      "[Prompt mappings] 275 lines\n",
      "\n",
      "[Server usage] prompt_tokens = 20351, completion_tokens = 8251, total_tokens = 28602\n",
      "\n",
      "[Step 2] Evaluating predictions...\n",
      "\n",
      "=== Q5 ===\n",
      "Question: A regional oceanographic institute based in Indonesia enters into a public-private partnership with a biotechnology start-up from South Africa to collect marine genetic resources (MGRs) of areas beyond national jurisdiction. The samples are stored in Indonesia and later transferred to South Africa for processing and sequencing. The resulting digital sequence information (DSI) is deposited in a proprietary database managed by the private Indonesian partner. A commercial product is eventually developed using inputs. No prior informed consent (PIC) was obtained from any Indigenous Peoples and Local Communities (IPLCs), and no benefit-sharing measures were agreed upon. The Indonesian State remains unaware of the downstream commercialisation. Question: Identify substantive provisions that allocate obligations to users and States when MGRs are collected in ABNJ, transferred between jurisdictions, and later used in commercial applications involving DSI.\n",
      "Gold (14): ['BBNJ_M021', 'BBNJ_M032', 'BBNJ_M035', 'BBNJ_M037', 'BBNJ_M038', 'BBNJ_M039', 'BBNJ_M040', 'BBNJ_M043', 'BBNJ_M044', 'BBNJ_M045', 'BBNJ_M046', 'BBNJ_M047', 'BBNJ_M049', 'BBNJ_M050']\n",
      "Preds(22): ['BBNJ_M022', 'BBNJ_M023', 'BBNJ_M024', 'BBNJ_M025', 'BBNJ_M026', 'BBNJ_M027', 'BBNJ_M028', 'BBNJ_M029', 'BBNJ_M030', 'BBNJ_M031', 'BBNJ_M033', 'BBNJ_M034', 'BBNJ_M035', 'BBNJ_M036', 'BBNJ_M037', 'BBNJ_M038', 'BBNJ_M039', 'BBNJ_M040', 'BBNJ_M043', 'BBNJ_M044', 'BBNJ_M045', 'BBNJ_M047']\n",
      "\n",
      "[NLTK metrics]\n",
      " Precision=0.409, Recall=0.643, F1=0.500\n",
      "\n",
      "[Manual metrics from TP/FP/FN]\n",
      " Precision=0.409, Recall=0.643, F1=0.500\n",
      " Counts -> TP=9, FP=13, FN=5\n",
      "\n",
      "--- Processing Question Q6 ---\n",
      "[Filtered] Using: /Users/rohanpersonal/Desktop/jurix_paper/normalized_mappings/final_files/results_actor_filtering_HW/gpt-5_high/question_6_filtered_rows.csv (rows=271)\n",
      "\n",
      "[Step 1] Retrieving predicted UIDs...\n",
      "[Prompt mappings] 271 lines\n",
      "\n",
      "[Server usage] prompt_tokens = 20068, completion_tokens = 9601, total_tokens = 29669\n",
      "\n",
      "[Step 2] Evaluating predictions...\n",
      "\n",
      "=== Q6 ===\n",
      "Question: A university in Sri Lanka conducts joint research with a Malaysian counterpart under a Memorandum of Understanding (MoU) that frames the activity as non-commercial. The project accesses marine genetic resources (MGRs) from Sri Lanka’s territorial sea and generates digital sequence information (DSI) stored in a shared repository. Years later, a private Malaysian pharmaceutical company builds upon the DSI to develop a patented molecule. Since the original access was classified as non-commercial, no mutually agreed terms (MAT) or tracking measures were put in place. The Sri Lankan authorities and associated community actors are not informed. Question: Identify substantive provisions that define the legal responsibilities of research institutions and States in ensuring benefit-sharing when DSI arising from publicly funded research is later used commercially.\n",
      "Gold (14): ['NAGOYA_M003', 'NAGOYA_M011', 'NAGOYA_M013', 'NAGOYA_M015', 'NAGOYA_M019', 'NAGOYA_M045', 'NAGOYA_M046', 'NAGOYA_M047', 'NAGOYA_M051', 'NAGOYA_M052', 'NAGOYA_M053', 'NAGOYA_M054', 'NAGOYA_M055', 'NAGOYA_M056']\n",
      "Preds(9): ['CBD_M049', 'NAGOYA_M003', 'NAGOYA_M013', 'NAGOYA_M015', 'NAGOYA_M019', 'NAGOYA_M045', 'NAGOYA_M046', 'NAGOYA_M051', 'NAGOYA_M052']\n",
      "\n",
      "[NLTK metrics]\n",
      " Precision=0.889, Recall=0.571, F1=0.696\n",
      "\n",
      "[Manual metrics from TP/FP/FN]\n",
      " Precision=0.889, Recall=0.571, F1=0.696\n",
      " Counts -> TP=8, FP=1, FN=6\n",
      "\n",
      "--- Processing Question Q7 ---\n",
      "[Filtered] Using: /Users/rohanpersonal/Desktop/jurix_paper/normalized_mappings/final_files/results_actor_filtering_HW/gpt-5_high/question_7_filtered_rows.csv (rows=264)\n",
      "\n",
      "[Step 1] Retrieving predicted UIDs...\n",
      "[Prompt mappings] 264 lines\n",
      "\n",
      "[Server usage] prompt_tokens = 19633, completion_tokens = 10883, total_tokens = 30516\n",
      "\n",
      "[Step 2] Evaluating predictions...\n",
      "\n",
      "=== Q7 ===\n",
      "Question: A marine research centre in Mozambique issues a permit to a domestic university to collect marine genetic resources (MGRs) in its EEZ. The researchers rely on traditional knowledge previously gathered from coastal communities in Madagascar, without disclosing this information during the permit application. No prior informed consent (PIC) was obtained from the Malagasy communities, and no benefit-sharing provisions are included in the national access authorisation. The resulting bioactive compounds are later used in collaborative research with a French institution. Question: Identify substantive provisions that establish obligations for States and users concerning the use of TK originating in another jurisdiction, when incorporated into activities authorised under national ABS frameworks.\n",
      "Gold (12): ['CBD_M019', 'NAGOYA_M013', 'NAGOYA_M014', 'NAGOYA_M016', 'NAGOYA_M026', 'NAGOYA_M045', 'NAGOYA_M046', 'NAGOYA_M047', 'NAGOYA_M048', 'NAGOYA_M051', 'NAGOYA_M052', 'NAGOYA_M053']\n",
      "Preds(8): ['CBD_M019', 'NAGOYA_M006', 'NAGOYA_M016', 'NAGOYA_M017', 'NAGOYA_M048', 'NAGOYA_M049', 'NAGOYA_M050', 'NAGOYA_M051']\n",
      "\n",
      "[NLTK metrics]\n",
      " Precision=0.500, Recall=0.333, F1=0.400\n",
      "\n",
      "[Manual metrics from TP/FP/FN]\n",
      " Precision=0.500, Recall=0.333, F1=0.400\n",
      " Counts -> TP=4, FP=4, FN=8\n",
      "\n",
      "--- Processing Question Q8 ---\n",
      "[Filtered] Using: /Users/rohanpersonal/Desktop/jurix_paper/normalized_mappings/final_files/results_actor_filtering_HW/gpt-5_high/question_8_filtered_rows.csv (rows=344)\n",
      "\n",
      "[Step 1] Retrieving predicted UIDs...\n",
      "[Prompt mappings] 344 lines\n",
      "\n",
      "[Server usage] prompt_tokens = 24547, completion_tokens = 9873, total_tokens = 34420\n",
      "\n",
      "[Step 2] Evaluating predictions...\n",
      "\n",
      "=== Q8 ===\n",
      "Question: An open-access repository maintained by a government-funded project in the Philippines publishes DSI generated from MGRs collected in the country’s EEZ. A private company based in Nigeria accesses the DSI and uses it to develop a novel biofertiliser for commercial use. The original research did not include downstream tracking, and no benefit-sharing was envisaged. The Philippine State becomes aware of the product only after commercialisation has already begun. Question: Identify substantive provisions that regulate benefit-sharing in scenarios where DSI published by a public institution is later used by private foreign users, especially in the absence of access conditions or notification requirements.\n",
      "Gold (14): ['CBD_M049', 'NAGOYA_M003', 'NAGOYA_M013', 'NAGOYA_M015', 'NAGOYA_M019', 'NAGOYA_M045', 'NAGOYA_M046', 'NAGOYA_M047', 'NAGOYA_M051', 'NAGOYA_M052', 'NAGOYA_M053', 'NAGOYA_M054', 'NAGOYA_M055', 'NAGOYA_M056']\n",
      "Preds(10): ['CBD_M049', 'NAGOYA_M003', 'NAGOYA_M005', 'NAGOYA_M013', 'NAGOYA_M015', 'NAGOYA_M019', 'NAGOYA_M045', 'NAGOYA_M046', 'NAGOYA_M047', 'NAGOYA_M051']\n",
      "\n",
      "[NLTK metrics]\n",
      " Precision=0.900, Recall=0.643, F1=0.750\n",
      "\n",
      "[Manual metrics from TP/FP/FN]\n",
      " Precision=0.900, Recall=0.643, F1=0.750\n",
      " Counts -> TP=9, FP=1, FN=5\n",
      "\n",
      "--- Processing Question Q9 ---\n",
      "[Filtered] Using: /Users/rohanpersonal/Desktop/jurix_paper/normalized_mappings/final_files/results_actor_filtering_HW/gpt-5_high/question_9_filtered_rows.csv (rows=369)\n",
      "\n",
      "[Step 1] Retrieving predicted UIDs...\n",
      "[Prompt mappings] 369 lines\n",
      "\n",
      "[Server usage] prompt_tokens = 26048, completion_tokens = 11667, total_tokens = 37715\n",
      "\n",
      "[Step 2] Evaluating predictions...\n",
      "\n",
      "=== Q9 ===\n",
      "Question: A regional research programme involving Cuba, Ghana, and Venezuela aims to document and catalogue marine biodiversity hotspots across their respective EEZs. While Cuba and Ghana require prior informed consent (PIC) and mutually agreed terms (MAT) for access to marine genetic resources, Venezuela does not. Samples collected in Venezuelan waters are processed in Ghana and sequenced in Cuba. A regional database is created, but no harmonised approach to benefit-sharing is adopted. Question: Identify substantive provisions (duty, claim-right, power, liability, immunity) applicable to regional cooperation projects involving MGR access under variable national ABS frameworks, particularly where DSI and biological materials are exchanged across jurisdictions.\n",
      "Gold (17): ['CBD_M044', 'CBD_M046', 'CBD_M047', 'CBD_M049', 'NAGOYA_M003', 'NAGOYA_M011', 'NAGOYA_M013', 'NAGOYA_M015', 'NAGOYA_M045', 'NAGOYA_M046', 'NAGOYA_M047', 'NAGOYA_M051', 'NAGOYA_M052', 'NAGOYA_M053', 'NAGOYA_M054', 'NAGOYA_M055', 'NAGOYA_M056']\n",
      "Preds(24): ['CBD_M045', 'CBD_M046', 'CBD_M047', 'CBD_M048', 'CBD_M049', 'NAGOYA_M003', 'NAGOYA_M007', 'NAGOYA_M010', 'NAGOYA_M011', 'NAGOYA_M012', 'NAGOYA_M013', 'NAGOYA_M015', 'NAGOYA_M019', 'NAGOYA_M024', 'NAGOYA_M040', 'NAGOYA_M041', 'NAGOYA_M042', 'NAGOYA_M045', 'NAGOYA_M046', 'NAGOYA_M047', 'NAGOYA_M051', 'NAGOYA_M052', 'NAGOYA_M058', 'NAGOYA_M060']\n",
      "\n",
      "[NLTK metrics]\n",
      " Precision=0.500, Recall=0.706, F1=0.585\n",
      "\n",
      "[Manual metrics from TP/FP/FN]\n",
      " Precision=0.500, Recall=0.706, F1=0.585\n",
      " Counts -> TP=12, FP=12, FN=5\n",
      "\n",
      "--- Processing Question Q10 ---\n",
      "[Filtered] Using: /Users/rohanpersonal/Desktop/jurix_paper/normalized_mappings/final_files/results_actor_filtering_HW/gpt-5_high/question_10_filtered_rows.csv (rows=341)\n",
      "\n",
      "[Step 1] Retrieving predicted UIDs...\n",
      "[Prompt mappings] 341 lines\n",
      "\n",
      "[Server usage] prompt_tokens = 24384, completion_tokens = 8857, total_tokens = 33241\n",
      "\n",
      "[Step 2] Evaluating predictions...\n",
      "\n",
      "=== Q10 ===\n",
      "Question: Kenya and Tanzania sign a bilateral cooperation agreement allowing joint research on marine genetic resources (MGRs) in their EEZs. Under the agreement, MGR samples collected in Tanzania are transferred to Kenya for sequencing and archived in a national biobank. Years later, a Kenyan start-up uses those materials to develop a high-performance industrial enzyme, with no notification to Tanzanian authorities or benefit-sharing mechanism. Question: Identify substantive provisions that govern the downstream commercial use of MGRs transferred between Parties under bilateral cooperation frameworks, including implications for benefit-sharing and traceability.\n",
      "Gold (16): ['CBD_M044', 'CBD_M046', 'CBD_M047', 'CBD_M049', 'NAGOYA_M003', 'NAGOYA_M013', 'NAGOYA_M015', 'NAGOYA_M045', 'NAGOYA_M046', 'NAGOYA_M047', 'NAGOYA_M051', 'NAGOYA_M052', 'NAGOYA_M053', 'NAGOYA_M054', 'NAGOYA_M055', 'NAGOYA_M056']\n",
      "Preds(13): ['CBD_M049', 'NAGOYA_M001', 'NAGOYA_M002', 'NAGOYA_M003', 'NAGOYA_M013', 'NAGOYA_M015', 'NAGOYA_M019', 'NAGOYA_M042', 'NAGOYA_M045', 'NAGOYA_M046', 'NAGOYA_M047', 'NAGOYA_M051', 'NAGOYA_M052']\n",
      "\n",
      "[NLTK metrics]\n",
      " Precision=0.692, Recall=0.562, F1=0.621\n",
      "\n",
      "[Manual metrics from TP/FP/FN]\n",
      " Precision=0.692, Recall=0.562, F1=0.621\n",
      " Counts -> TP=9, FP=4, FN=7\n",
      "\n",
      "--- Processing Question Q11 ---\n",
      "[Filtered] Using: /Users/rohanpersonal/Desktop/jurix_paper/normalized_mappings/final_files/results_actor_filtering_HW/gpt-5_high/question_11_filtered_rows.csv (rows=364)\n",
      "\n",
      "[Step 1] Retrieving predicted UIDs...\n",
      "[Prompt mappings] 364 lines\n",
      "\n",
      "[Server usage] prompt_tokens = 25724, completion_tokens = 8748, total_tokens = 34472\n",
      "\n",
      "[Step 2] Evaluating predictions...\n",
      "\n",
      "=== Q11 ===\n",
      "Question: A Vietnamese research team collects marine genetic resources in its EEZ and uses traditional knowledge from a coastal ethnic minority to target specific organisms. The researchers publish DSI based on the collected material and acknowledge institutional affiliations but make no reference to the community that contributed the knowledge. No PIC is obtained, and no benefits are returned to the knowledge holders. Years later, the DSI becomes foundational in a regional bioeconomy initiative. The community remains unrecognised. Question: Identify substantive provisions  that establish responsibilities for acknowledging TK contributions and sharing benefits when DSI and downstream innovations result from undocumented community knowledge.\n",
      "Gold (14): ['CBD_M019', 'CBD_M049', 'NAGOYA_M006', 'NAGOYA_M013', 'NAGOYA_M014', 'NAGOYA_M016', 'NAGOYA_M026', 'NAGOYA_M045', 'NAGOYA_M046', 'NAGOYA_M047', 'NAGOYA_M048', 'NAGOYA_M051', 'NAGOYA_M052', 'NAGOYA_M053']\n",
      "Preds(5): ['CBD_M019', 'CBD_M056', 'NAGOYA_M006', 'NAGOYA_M016', 'NAGOYA_M048']\n",
      "\n",
      "[NLTK metrics]\n",
      " Precision=0.800, Recall=0.286, F1=0.421\n",
      "\n",
      "[Manual metrics from TP/FP/FN]\n",
      " Precision=0.800, Recall=0.286, F1=0.421\n",
      " Counts -> TP=4, FP=1, FN=10\n",
      "\n",
      "--- Processing Question Q12 ---\n",
      "[Filtered] Using: /Users/rohanpersonal/Desktop/jurix_paper/normalized_mappings/final_files/results_actor_filtering_HW/gpt-5_high/question_12_filtered_rows.csv (rows=260)\n",
      "\n",
      "[Step 1] Retrieving predicted UIDs...\n",
      "[Prompt mappings] 260 lines\n",
      "\n",
      "[Server usage] prompt_tokens = 19376, completion_tokens = 8531, total_tokens = 27907\n",
      "\n",
      "[Step 2] Evaluating predictions...\n",
      "\n",
      "=== Q12 ===\n",
      "Question: An Indian marine biology institute collects MGRs in its EEZ and submits samples to a regional repository in the Seychelles for long-term storage. Without the Indian authority’s knowledge, a Colombian research centre later accesses the samples under the repository’s terms and develops a commercially viable marine antioxidant. The repository requires minimal documentation and enforces no benefit-sharing obligations. India’s ABS authorities become aware only post-commercialisation. Question: Identify substantive provisions that regulate the responsibility of States and third-party repositories for ensuring benefit-sharing when MGRs from national jurisdiction are shared beyond their control.\n",
      "Gold (16): ['CBD_M044', 'CBD_M046', 'CBD_M047', 'CBD_M049', 'NAGOYA_M003', 'NAGOYA_M013', 'NAGOYA_M015', 'NAGOYA_M045', 'NAGOYA_M046', 'NAGOYA_M047', 'NAGOYA_M051', 'NAGOYA_M052', 'NAGOYA_M053', 'NAGOYA_M054', 'NAGOYA_M055', 'NAGOYA_M056']\n",
      "Preds(10): ['NAGOYA_M003', 'NAGOYA_M005', 'NAGOYA_M013', 'NAGOYA_M015', 'NAGOYA_M042', 'NAGOYA_M045', 'NAGOYA_M046', 'NAGOYA_M047', 'NAGOYA_M051', 'NAGOYA_M052']\n",
      "\n",
      "[NLTK metrics]\n",
      " Precision=0.800, Recall=0.500, F1=0.615\n",
      "\n",
      "[Manual metrics from TP/FP/FN]\n",
      " Precision=0.800, Recall=0.500, F1=0.615\n",
      " Counts -> TP=8, FP=2, FN=8\n",
      "\n",
      "--- Processing Question Q13 ---\n",
      "[Filtered] Using: /Users/rohanpersonal/Desktop/jurix_paper/normalized_mappings/final_files/results_actor_filtering_HW/gpt-5_high/question_13_filtered_rows.csv (rows=370)\n",
      "\n",
      "[Step 1] Retrieving predicted UIDs...\n",
      "[Prompt mappings] 370 lines\n",
      "\n",
      "[Server usage] prompt_tokens = 26089, completion_tokens = 10486, total_tokens = 36575\n",
      "\n",
      "[Step 2] Evaluating predictions...\n",
      "\n",
      "=== Q13 ===\n",
      "Question: During an international marine science symposium held in Colombia, a community representative from Papua New Guinea informally shares traditional knowledge regarding certain marine species. A research team from Brazil later collects MGRs in ABNJ consistent with the TK and publishes the resulting DSI. A Brazilian cosmetics company commercialises one of the compounds. No PIC or MAT were ever established, and the original knowledge-sharing context was not recorded. Question: Identify substantive provisions  that apply when TK is disclosed in informal international settings and subsequently used to guide MGR collection and DSI-based innovation, particularly in ABNJ.\n",
      "Gold (24): ['BBNJ_M021', 'BBNJ_M032', 'BBNJ_M035', 'BBNJ_M037', 'BBNJ_M038', 'BBNJ_M039', 'BBNJ_M040', 'BBNJ_M043', 'BBNJ_M044', 'BBNJ_M045', 'BBNJ_M046', 'BBNJ_M047', 'CBD_M019', 'NAGOYA_M006', 'NAGOYA_M014', 'NAGOYA_M016', 'NAGOYA_M026', 'NAGOYA_M045', 'NAGOYA_M046', 'NAGOYA_M047', 'NAGOYA_M048', 'NAGOYA_M051', 'NAGOYA_M052', 'NAGOYA_M053']\n",
      "Preds(20): ['BBNJ_M021', 'BBNJ_M022', 'BBNJ_M023', 'BBNJ_M031', 'BBNJ_M034', 'BBNJ_M035', 'BBNJ_M037', 'BBNJ_M039', 'BBNJ_M040', 'BBNJ_M041', 'BBNJ_M044', 'BBNJ_M045', 'BBNJ_M047', 'CBD_M019', 'NAGOYA_M006', 'NAGOYA_M016', 'NAGOYA_M027', 'NAGOYA_M048', 'NAGOYA_M049', 'NAGOYA_M051']\n",
      "\n",
      "[NLTK metrics]\n",
      " Precision=0.650, Recall=0.542, F1=0.591\n",
      "\n",
      "[Manual metrics from TP/FP/FN]\n",
      " Precision=0.650, Recall=0.542, F1=0.591\n",
      " Counts -> TP=13, FP=7, FN=11\n",
      "\n",
      "--- Processing Question Q14 ---\n",
      "[Filtered] Using: /Users/rohanpersonal/Desktop/jurix_paper/normalized_mappings/final_files/results_actor_filtering_HW/gpt-5_high/question_14_filtered_rows.csv (rows=364)\n",
      "\n",
      "[Step 1] Retrieving predicted UIDs...\n",
      "[Prompt mappings] 364 lines\n",
      "\n",
      "[Server usage] prompt_tokens = 25729, completion_tokens = 10470, total_tokens = 36199\n",
      "\n",
      "[Step 2] Evaluating predictions...\n",
      "\n",
      "=== Q14 ===\n",
      "Question: A marine research consortium in Senegal obtains access permits to sample MGRs from multiple West African EEZs (The Gambia, Guinea-Bissau, and Sierra Leone). However, each national permit is granted independently, and no regional coordination exists to track duplicated collections, sample overlaps, or shared benefit-sharing obligations. Some species are sampled multiple times under different permits, and the DSI generated is pooled into a single research platform. Question: Identify substantive provisions that address duplicative access to MGRs across multiple national permits, including obligations for States and users to ensure benefit-sharing and transparency in cross-border collection scenarios.\n",
      "Gold (13): ['CBD_M044', 'CBD_M046', 'CBD_M047', 'CBD_M049', 'NAGOYA_M003', 'NAGOYA_M013', 'NAGOYA_M015', 'NAGOYA_M045', 'NAGOYA_M046', 'NAGOYA_M047', 'NAGOYA_M051', 'NAGOYA_M052', 'NAGOYA_M053']\n",
      "Preds(13): ['CBD_M049', 'NAGOYA_M003', 'NAGOYA_M007', 'NAGOYA_M013', 'NAGOYA_M015', 'NAGOYA_M023', 'NAGOYA_M024', 'NAGOYA_M042', 'NAGOYA_M045', 'NAGOYA_M046', 'NAGOYA_M047', 'NAGOYA_M051', 'NAGOYA_M052']\n",
      "\n",
      "[NLTK metrics]\n",
      " Precision=0.692, Recall=0.692, F1=0.692\n",
      "\n",
      "[Manual metrics from TP/FP/FN]\n",
      " Precision=0.692, Recall=0.692, F1=0.692\n",
      " Counts -> TP=9, FP=4, FN=4\n",
      "\n",
      "--- Processing Question Q15 ---\n",
      "[Filtered] Using: /Users/rohanpersonal/Desktop/jurix_paper/normalized_mappings/final_files/results_actor_filtering_HW/gpt-5_high/question_15_filtered_rows.csv (rows=351)\n",
      "\n",
      "[Step 1] Retrieving predicted UIDs...\n",
      "[Prompt mappings] 351 lines\n",
      "\n",
      "[Server usage] prompt_tokens = 24949, completion_tokens = 6142, total_tokens = 31091\n",
      "\n",
      "[Step 2] Evaluating predictions...\n",
      "\n",
      "=== Q15 ===\n",
      "Question: A private marine biotech company in India funds a university research project focused on sequencing MGRs from India’s EEZ. The university signs access agreements under its non-commercial status and omits mention of its private partner. After sequencing, the company claims the DSI for product development. The national ABS authority had no knowledge of the private funder, and no MAT or benefit-sharing terms were negotiated for commercial use. This raises concerns over misrepresentation of research intent and hidden industry-academic collaboration. Question: Identify substantive provisions that govern the disclosure of user identity and intent in access agreements involving MGRs, particularly when institutional users are acting on behalf of undisclosed commercial partners.\n",
      "Gold (17): ['CBD_M044', 'CBD_M046', 'CBD_M047', 'CBD_M049', 'NAGOYA_M003', 'NAGOYA_M013', 'NAGOYA_M015', 'NAGOYA_M019', 'NAGOYA_M045', 'NAGOYA_M046', 'NAGOYA_M047', 'NAGOYA_M051', 'NAGOYA_M052', 'NAGOYA_M053', 'NAGOYA_M054', 'NAGOYA_M055', 'NAGOYA_M056']\n",
      "Preds(7): ['NAGOYA_M012', 'NAGOYA_M013', 'NAGOYA_M015', 'NAGOYA_M019', 'NAGOYA_M042', 'NAGOYA_M051', 'NAGOYA_M052']\n",
      "\n",
      "[NLTK metrics]\n",
      " Precision=0.714, Recall=0.294, F1=0.417\n",
      "\n",
      "[Manual metrics from TP/FP/FN]\n",
      " Precision=0.714, Recall=0.294, F1=0.417\n",
      " Counts -> TP=5, FP=2, FN=12\n",
      "\n",
      "=== Aggregating results into DataFrame ===\n",
      "\n",
      "=== MACRO AVERAGE over questions ===\n",
      "Precision=0.746, Recall=0.502, F1=0.573\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Q1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q2</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q3</th>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q4</th>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.477612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q5</th>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q6</th>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.695652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q7</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q8</th>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q9</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.585366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q10</th>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.620690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q11</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.421053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q12</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.615385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q13</th>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.590909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q14</th>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q15</th>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Precision    Recall        F1\n",
       "question_id                               \n",
       "Q1            1.000000  0.333333  0.500000\n",
       "Q2            0.800000  0.571429  0.666667\n",
       "Q3            0.900000  0.529412  0.666667\n",
       "Q4            0.941176  0.320000  0.477612\n",
       "Q5            0.409091  0.642857  0.500000\n",
       "Q6            0.888889  0.571429  0.695652\n",
       "Q7            0.500000  0.333333  0.400000\n",
       "Q8            0.900000  0.642857  0.750000\n",
       "Q9            0.500000  0.705882  0.585366\n",
       "Q10           0.692308  0.562500  0.620690\n",
       "Q11           0.800000  0.285714  0.421053\n",
       "Q12           0.800000  0.500000  0.615385\n",
       "Q13           0.650000  0.541667  0.590909\n",
       "Q14           0.692308  0.692308  0.692308\n",
       "Q15           0.714286  0.294118  0.416667"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PIPELINE END ===\n"
     ]
    }
   ],
   "source": [
    "# --------------- HOHFELDIAN DEFINITIONS ---------------\n",
    "HOHFELDIAN_TEXT = \"\"\"Basic Legal Positions:\n",
    "• Claim-Right: X has a right that Y must (or must not) do something.\n",
    "• Duty/Obligation: Y is required to act (or refrain) because X has a claim-right.\n",
    "• Liberty/Privilege: X may act without Y having the right to prevent it.\n",
    "• No-Right: Y has no claim-right against X’s action.\n",
    "\n",
    "Second-Order Legal Positions:\n",
    "• Power: X can change legal relations (e.g., create, alter, or extinguish rights).\n",
    "• Liability: Y is subject to X’s exercise of power.\n",
    "• Immunity: X is protected from changes to their legal relations.\n",
    "• Disability: Y lacks the power to change X’s legal relations.\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an expert in international law across the Biodiversity Beyond National Jurisdiction (BBNJ) Agreement, the Convention on Biological Diversity (CBD), and the Nagoya Protocol.\n",
    "\n",
    "Glossary of key abbreviations:\n",
    "- ABNJ = Areas Beyond National Jurisdiction\n",
    "- GR = Genetic Resources.\n",
    "- MGR = Marine Genetic Resources.\n",
    "- DSI = Digital Sequence Information\n",
    "- TK = Traditional Knowledge.\n",
    "- IPLC = Indigenous Peoples and Local Communities.\n",
    "- PIC = Prior Informed Consent.\n",
    "- MAT = Mutually Agreed Terms.\n",
    "- R&D = Research and Development.\n",
    "\n",
    "You will receive: (1) actor definitions, (2) Hohfeldian definitions, (3) a list of treaty mappings as natural-language statements with Unique_IDs, and (4) ONE fact pattern question.\n",
    "\n",
    "TASK\n",
    "Return ONLY the Unique_IDs of mapping rows that DIRECTLY allocate legal positions (duty, claim-right, power, liability, immunity, disability, liberty, no-right) to the actors implicated by the fact pattern.\n",
    "\n",
    "INTERNAL METHOD (do not reveal notes or steps)\n",
    "• Parse the fact pattern into facets: actors; jurisdiction (national jurisdiction vs ABNJ; any cross-border transfers; repositories); resource/data types (MGR/GR, derivatives, DSI); TK/IPLC provenance; lifecycle stage(s) (access, publication, utilisation/R&D, transfer/repository, monitoring/checkpoints/traceability, downstream commercialisation); PIC/MAT/benefit-sharing posture; disclosure/notification posture.\n",
    "• Jurisdictional gating (apply strictly):\n",
    "  – If the activity is solely within national jurisdiction, consider national-jurisdiction regimes; do NOT include ABNJ-specific rows unless the pattern explicitly involves ABNJ.\n",
    "  – If the activity is solely in ABNJ, consider ABNJ-specific rows; do NOT import national-jurisdiction rows unless the pattern explicitly invokes access/TK within national jurisdiction.\n",
    "  – If both zones are implicated, include both sets where each zone’s obligations are triggered.\n",
    "• Allocative filter (hard): keep rows that impose or allocate concrete legal positions to identified actors (e.g., “shall/must/required to/entitled to/liable to”) for the relevant lifecycle facet. Drop rows that are purely objectives, principles, scope/definitions, general cooperation/capacity/technology-transfer, finance, or institution-building unless they impose a concrete obligation tied to the specific fact pattern.\n",
    "• Specificity preference: prefer rows that explicitly match (i) the exact jurisdictional setting (national vs ABNJ), (ii) resource/data type (MGR/GR/derivatives/DSI), (iii) TK involvement, and (iv) the lifecycle facet(s) implicated. \n",
    "• Consolidate near-duplicates by keeping the most specific.\n",
    "\n",
    "RANKING & OUTPUT\n",
    "• Deduplicate and rank by: (1) directness to the fact pattern, (2) specificity to jurisdiction and resource/data type, (3) relevance to the lifecycle facet(s), (4) clarity of Hohfeldian allocation.\n",
    "• Output STRICT JSON ONLY (no explanations):\n",
    "{\"ranked_uids\": [\"UID_1\", \"UID_2\", \"...\"]}\n",
    "• Do not invent IDs; return an empty list if nothing directly applies.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "USER_TEMPLATE_GLOBAL = \"\"\"Actor Definitions (table):\n",
    "{actors}\n",
    "\n",
    "Hohfeldian Definitions:\n",
    "{hohfeld}\n",
    "\n",
    "Mappings (UID, provision, NLS):\n",
    "{mappings}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Return ONLY strict JSON:\n",
    "{{\"ranked_uids\": [\"UID_1\", \"UID_2\", \"...\"]}}\n",
    "\"\"\"\n",
    "\n",
    "# --------------- JSON PARSING HELPER \n",
    "#Extract the JSON block from LLM's output \n",
    "# unchanged from normal retrieval pipeline\n",
    "def extract_json_dict(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Try to extract a JSON dictionary from a string.\n",
    "    Useful when the LLM response contains extra text around a JSON object.\n",
    "    \"\"\"\n",
    "    # Step 1: Use regex to search for the first {...} block in the text\n",
    "    # re.S flag makes '.' also match newlines (so JSON spanning multiple lines works)\n",
    "    m = re.search(r\"\\{.*\\}\", text, flags=re.S)\n",
    "    if not m:   # If no {...} block was found, return empty dict\n",
    "        return {}\n",
    "    try:  # Step 2: Try to parse the matched substring (the full {...}) as JSON\n",
    "        return json.loads(m.group(0))\n",
    "    except Exception:\n",
    "        try:\n",
    "            # Step 3 (fallback): Sometimes LLMs generate trailing commas before ]\n",
    "            # Example: {\"a\": [1,2,], \"b\": 3}\n",
    "            # That is invalid JSON, so we remove \", ]\" → \"]\"\n",
    "            cleaned = re.sub(r\",\\s*]\", \"]\", m.group(0))\n",
    "            return json.loads(cleaned)\n",
    "        except Exception:\n",
    "            return {}\n",
    "\n",
    "# --------------- OPENAI CALL WRAPPER (now OpenRouter) ---------------\n",
    "# unchanged from normal retrieval pipeline\n",
    "def call_llm(system_prompt: str, user_prompt: str) -> dict:\n",
    "    \"\"\"\n",
    "    Call the model via OpenRouter and return parsed JSON (dict). If parsing fails, returns {}.\n",
    "    \"\"\"\n",
    "    # Build messages for both counting and the actual call\n",
    "    messages = [\n",
    "        {\"role\":\"system\",\"content\":system_prompt},\n",
    "        {\"role\":\"user\",\"content\":user_prompt}\n",
    "    ]\n",
    "\n",
    "    #estimate prompt (input) tokens via LiteLLM (for this single model)\n",
    "    est_tokens = estimate_tokens_with_litellm(messages, MODEL_NAME)\n",
    "    #print(f\"\\n\\n[Token estimate] {MODEL_NAME}: prompt_tokens ≈ {est_tokens}\")\n",
    "\n",
    "    # Make the actual request to LLM\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=messages,\n",
    "    extra_body={\n",
    "            \"transforms\": [],\n",
    "            \"seed\": 42,\n",
    "            \"response_format\": {\"type\": \"json_object\"},\n",
    "            \"provider\": {\"allow_fallbacks\": False},\n",
    "            \"reasoning\": {\"effort\": \"high\", \"exclude\": True} \n",
    "        },\n",
    "    )\n",
    "    \n",
    "    # If you want server-accounted tokens too:\n",
    "    try:\n",
    "        server_prompt_tokens = resp.usage.prompt_tokens #after response from the server\n",
    "        print(f\"\\n[Server usage] prompt_tokens = {server_prompt_tokens}, completion_tokens = {resp.usage.completion_tokens}, total_tokens = {resp.usage.total_tokens}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    text = resp.choices[0].message.content\n",
    "    \n",
    "    #print(f\"\\nLLM's raw output: {text}\")\n",
    "    #print(f\"\\nLLM's output after extract_json_dict(): {extract_json_dict(text or \"\")}\")\n",
    "    return extract_json_dict(text or \"\")\n",
    "\n",
    "\n",
    "\n",
    "# =========================\n",
    "# --- NEW HELPERS for per-question filtered files ---\n",
    "# =========================\n",
    "def _infer_question_number(question_id: str, default_n: int) -> int:\n",
    "    \"\"\"\n",
    "    Extract a number from question_id (e.g., 'Q7' -> 7). If none found, fallback to default_n.\n",
    "    \"\"\"\n",
    "    m = re.search(r\"(\\d+)\", str(question_id))\n",
    "    return int(m.group(1)) if m else int(default_n)\n",
    "\n",
    "def _filtered_path_for(n: int) -> str:\n",
    "    \"\"\"Build the path to question_n_filtered_rows.csv\"\"\"\n",
    "    return os.path.join(FILTERED_MAPPINGS_DIR, FILTERED_FILENAME_TEMPLATE.format(n=n))\n",
    "\n",
    "def load_filtered_mappings_for_question(n: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the per-question filtered mappings for question number n.\n",
    "    - Robust to delimiter (auto-sniff via engine='python', sep=None).\n",
    "    - Robust to column case (accepts 'unique_id' or 'Unique_ID').\n",
    "    - Normalizes 'provision' and 'natural_language_sentence'.\n",
    "    Returns a DataFrame with canonical columns:\n",
    "      ['Unique_ID', 'provision', 'natural_language_sentence']\n",
    "    \"\"\"\n",
    "    path = _filtered_path_for(n)\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Filtered file not found: {path}\")\n",
    "\n",
    "    # Let pandas sniff the delimiter & tolerate quoting (python engine)\n",
    "    # Also strip BOM if present (encoding='utf-8-sig') and keep text as str\n",
    "    df = pd.read_csv(path, sep=None, engine=\"python\", dtype=str, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # Case-insensitive column mapping\n",
    "    cols_map = {c.lower(): c for c in df.columns}\n",
    "\n",
    "    # Accept lowercase headers in filtered files\n",
    "    required_lower = [\"unique_id\", \"provision\", \"natural_language_sentence\"]\n",
    "    missing = [k for k in required_lower if k not in cols_map]\n",
    "    if missing:\n",
    "        raise KeyError(\n",
    "            f\"Missing required columns in filtered file {path}: {missing}. \"\n",
    "            f\"Found: {list(df.columns)}\"\n",
    "        )\n",
    "\n",
    "    # Rename to canonical column names expected by the rest of the pipeline\n",
    "    df = df.rename(columns={\n",
    "        cols_map[\"unique_id\"]: \"Unique_ID\",\n",
    "        cols_map[\"provision\"]: \"provision\",\n",
    "        cols_map[\"natural_language_sentence\"]: \"natural_language_sentence\",\n",
    "    })\n",
    "\n",
    "    # --- DEBUG PRINTS ---\n",
    "    #print(f\"Loaded filtered file for Q{n}: {path}\")\n",
    "    #print(\"Columns:\", list(df.columns))\n",
    "    #print(f\"Rows: {len(df)}\")\n",
    "    # show a quick peek so you can visually confirm content/format\n",
    "    #try:\n",
    "    #    print(df.head(3)[[\"Unique_ID\",\"provision\",\"natural_language_sentence\"]].to_string(index=False))\n",
    "    #    print(\"...\")\n",
    "    #    print(df.tail(2)[[\"Unique_ID\",\"provision\",\"natural_language_sentence\"]].to_string(index=False))\n",
    "    #except Exception:\n",
    "    # if any column missing for some reason, don't crash the run\n",
    "    #    pass\n",
    "    # --- END DEBUG PRINTS ---\n",
    "\n",
    "    \n",
    "    # Normalize text fields (same as load_mappings)\n",
    "    df[\"provision\"] = df[\"provision\"].apply(_norm)\n",
    "    df[\"natural_language_sentence\"] = df[\"natural_language_sentence\"].apply(_norm)\n",
    "\n",
    "    # Keep only the columns we actually use downstream\n",
    "    return df[[\"Unique_ID\", \"provision\", \"natural_language_sentence\"]]\n",
    "\n",
    "\n",
    "\n",
    "# =========================\n",
    "# RETRIEVAL (CHANGED: now takes per-question mapping_lines)\n",
    "# =========================\n",
    "def retrieve_ranked_uids_for_question(question_text: str, mapping_lines: list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Build one prompt with actor definitions, Hohfeldian definitions,\n",
    "    the PROVIDED mapping_lines (per-question filtered), and the question.\n",
    "    Return the FULL list of UIDs predicted by the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # quick visibility into the search space size & content\n",
    "    print(f\"[Prompt mappings] {len(mapping_lines)} lines\")\n",
    "    #if mapping_lines:\n",
    "    #    print(\"[Prompt mappings — first 2]\")\n",
    "    #    print(\"\\n\".join(mapping_lines[:2]))\n",
    "    #    if len(mapping_lines) > 2:\n",
    "    #        print(\"[Prompt mappings — last 2]\")\n",
    "    #        print(\"\\n\".join(mapping_lines[-2:]))\n",
    "\n",
    "    \n",
    "    user = USER_TEMPLATE_GLOBAL.format(\n",
    "        actors=ACTORS_TEXT,\n",
    "        hohfeld=HOHFELDIAN_TEXT,\n",
    "        mappings=\"\\n\".join(mapping_lines),    #there are now different mappings for each question\n",
    "        question=question_text\n",
    "    )\n",
    "\n",
    "    obj = call_llm(SYSTEM_PROMPT, user)\n",
    "    ranked = obj.get(\"ranked_uids\", [])\n",
    "    return [str(u).strip() for u in ranked if str(u).strip()]\n",
    "\n",
    "\n",
    "\n",
    "# unchanged from normal retrieval pipeline\n",
    "def eval_one(\n",
    "    question_id: str,\n",
    "    question_text: str,\n",
    "    gold_unique_ids: list[str],\n",
    "    ranked_pred_ids: list[str],\n",
    "    do_manual_counts: bool = True,   # toggle printing TP/FP/FN/ manual metrics\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate a single question:\n",
    "      - Computes precision, recall, F1 directly on sets (via NLTK).\n",
    "      - Optionally also computes TP, FP, FN, TN and manual Precision/Recall/F1\n",
    "        using a confusion matrix for transparency.\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalize to sets of clean strings\n",
    "    gold_set = {str(x).strip() for x in gold_unique_ids if str(x).strip()}\n",
    "    pred_set = {str(x).strip() for x in ranked_pred_ids if str(x).strip()}\n",
    "\n",
    "    # Metrics directly from NLTK\n",
    "    # NLTK returns None when a metric is undefined (e.g., no retrieved items → precision undefined).\n",
    "    # The `or 0.0` turns None into 0.0 so downstream code stays numeric and printable.\n",
    "    p_nltk = nltk_precision(gold_set, pred_set) or 0.0\n",
    "    r_nltk = nltk_recall(gold_set, pred_set) or 0.0\n",
    "    f1_nltk = nltk_f1(gold_set, pred_set) or 0.0\n",
    "\n",
    "    # ---- Optional: TP, FP, FN, TN counts + manual metrics ----\n",
    "    \n",
    "    tp = fp = fn = tn = 0\n",
    "    p_manual = r_manual = f1_manual = 0.0\n",
    "    if do_manual_counts:\n",
    "        # \"universe\" = union of gold and predicted IDs\n",
    "        U = list(gold_set | pred_set)\n",
    "\n",
    "        # Ground-truth and prediction boolean vectors over U\n",
    "        y_true = [u in gold_set for u in U]\n",
    "        y_pred = [u in pred_set for u in U]\n",
    "\n",
    "        # Confusion matrix → [TN, FP, FN, TP]\n",
    "        tn, fp, fn, tp = confusion_matrix(\n",
    "            y_true, y_pred, labels=[False, True]\n",
    "        ).ravel()\n",
    "\n",
    "        # Manual metrics from counts (safe divisions)\n",
    "        p_manual = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        r_manual = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1_manual = (\n",
    "            2 * p_manual * r_manual / (p_manual + r_manual)\n",
    "            if (p_manual + r_manual) > 0 else 0.0\n",
    "        )\n",
    "   \n",
    "    # ---- Print results ----\n",
    "    print(f\"\\n=== {question_id} ===\")\n",
    "    print(f\"Question: {question_text}\")\n",
    "    print(f\"Gold ({len(gold_set)}): {sorted(gold_set)}\")\n",
    "    print(f\"Preds({len(pred_set)}): {sorted(pred_set)}\")\n",
    "\n",
    "    # NLTK (set-based) metrics\n",
    "    print(\"\\n[NLTK metrics]\")\n",
    "    print(f\" Precision={p_nltk:.3f}, Recall={r_nltk:.3f}, F1={f1_nltk:.3f}\")\n",
    "    \n",
    "    # Manual (from TP/FP/FN) metrics + counts\n",
    "    if do_manual_counts:\n",
    "        print(\"\\n[Manual metrics from TP/FP/FN]\")\n",
    "        print(f\" Precision={p_manual:.3f}, Recall={r_manual:.3f}, F1={f1_manual:.3f}\")\n",
    "        print(f\" Counts -> TP={tp}, FP={fp}, FN={fn}\")\n",
    "    \n",
    "    # ---- Return results in a dict (row for DataFrame) ----\n",
    "    \n",
    "\n",
    "    return {\n",
    "        \"question_id\": question_id,\n",
    "        \"Precision\": p_nltk,\n",
    "        \"Recall\": r_nltk,\n",
    "        \"F1\": f1_nltk\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# RUN LOOP (CHANGED: load filtered mapping file per question)\n",
    "# =========================\n",
    "def run(questions, do_manual_counts: bool = True):\n",
    "    \"\"\"\n",
    "    For each question:\n",
    "      - Load its per-question filtered mapping CSV (question_{n}_filtered_rows.csv).\n",
    "      - Build mapping lines from that filtered set.\n",
    "      - Retrieve predicted UIDs with ONLY those lines.\n",
    "      - Evaluate against the gold set.\n",
    "      Note: Full mappings are never used here because we only load filtered rows for each question\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    print(\"=== PIPELINE START ===\")\n",
    "    for idx, q in enumerate(questions, start=1):\n",
    "        qid = q[\"question_id\"]\n",
    "        qtext = q[\"question_text\"]\n",
    "        gold = q[\"gold_unique_ids\"]\n",
    "        print(\"\\n--- Processing Question\", qid, \"---\")\n",
    "\n",
    "        # Pick the filtered file for this question\n",
    "        qnum = _infer_question_number(qid, default_n=idx)\n",
    "        filtered_path = _filtered_path_for(qnum)\n",
    "        #print(f\"Resolved question number: {qnum} → {filtered_path}\")\n",
    "        # Show the question text and gold\n",
    "        #print(\"[QUESTION TEXT]\", (qtext[:400] + \"…\") if len(qtext) > 400 else qtext)\n",
    "        #print(\"[GOLD UIDs]\", gold)\n",
    "\n",
    "        # 1) Load per-question filtered file (FAIL FAST if missing/malformed)\n",
    "        fdf = load_filtered_mappings_for_question(qnum)  # will raise if bad\n",
    "        print(f\"[Filtered] Using: {filtered_path} (rows={len(fdf)})\")\n",
    "        mapping_lines = build_mapping_lines(fdf)\n",
    "        if not mapping_lines:\n",
    "            raise AssertionError(f\"No mapping lines constructed for {qid} from {filtered_path}\")\n",
    "\n",
    "        # Step 1: Retrieval on the per-question mappings\n",
    "        print(\"\\n[Step 1] Retrieving predicted UIDs...\")\n",
    "        preds = retrieve_ranked_uids_for_question(qtext, mapping_lines)\n",
    "\n",
    "        # Step 2: Evaluation\n",
    "        print(\"\\n[Step 2] Evaluating predictions...\")\n",
    "        row = eval_one(\n",
    "            qid,\n",
    "            qtext,\n",
    "            gold,\n",
    "            preds,\n",
    "            do_manual_counts=do_manual_counts,\n",
    "        )\n",
    "        rows.append(row)\n",
    "\n",
    "    # Aggregate\n",
    "    print(\"\\n=== Aggregating results into DataFrame ===\")\n",
    "    df = pd.DataFrame(rows).set_index(\"question_id\")\n",
    "\n",
    "    #macro-average\n",
    "    macro = df[[\"Precision\", \"Recall\", \"F1\"]].mean().to_dict()\n",
    "    print(\"\\n=== MACRO AVERAGE over questions ===\")\n",
    "    print(f\"Precision={macro['Precision']:.3f}, \"\n",
    "          f\"Recall={macro['Recall']:.3f}, \"\n",
    "          f\"F1={macro['F1']:.3f}\")\n",
    "\n",
    "    try:\n",
    "        from IPython.display import display\n",
    "        display(df)\n",
    "    except Exception:\n",
    "        print(df)\n",
    "\n",
    "    print(\"\\n=== PIPELINE END ===\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# =========================\n",
    "# RUN\n",
    "# =========================w\n",
    "results_df = run(questions, do_manual_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d66fe8-ae11-4490-9544-4291a7867a92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
